\documentclass[11pt]{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
%\usepackage{fontspec}
\usepackage{amsfonts}
\usepackage{xeCJK}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{color}
\usepackage{listings}
%%plots
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{pgfplots}
\pgfplotsset{compat=newest} % Allows to place the legend below plot
\usepgfplotslibrary{units} % Allows to enter the units nicely
\setCJKmainfont{WenQuanYi Micro Hei}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\AuthorName}
\chead{\Class\ (\ClassInstructor\ \ClassTime): \Title}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\Title}{HW3}
\newcommand{\FinishDate}{\today}
\newcommand{\Class}{Machine Learning}
\newcommand{\ClassTime}{}
\newcommand{\ClassInstructor}{Professor Hung-Yi Lee}
\newcommand{\Department}{EE3}
\newcommand{\AuthorID}{b03901016}
\newcommand{\AuthorName}{陳昊}

%util
\newcommand{\horline}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\newcommand\n{\mbox{\qquad}}

%
% Title Page
%

\title{
{National Taiwan University}\\    
    \textmd{\textbf{\Class:\ \Title}}
}

\author{
	\Department \ \AuthorID \\
	\textbf{\AuthorName}
}
\date{
	\today \\
	\horline{1pt}
}



\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}
% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
%%%%%%%%%%%%%%Insert Code%%%%%%%%%%%%%%%%
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
    frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{document}
\maketitle
\section{Supervised Learning}
\n The first thing after read data from pickle file is to reshape it, to match the original (3072,) shape to (3, 32, 32) or (32, 32, 3) depends on different backend, so that the image can be used in a CNN network. Besides, I divide all the value in 255 so that the value in the data is normalize to [0,1].\\
\n The model that I got the best public score on Kaggle is a CNN network, which contains 3 convolution layers and 2 maxpooling layers, then connect to 3 fully connected hidden layers, shown below.\\
\n In addition to the CNN network, I also use \texttt{ImageDataGenerator} to simply generate more training data by randomly shift and rotate and flip the original images, this method increase the accuracy on testing data a lot. Without using \text{ImageDataGenerator} the best score I got on Kaggle public set is 0.60880, while I got an accuracy of 0.68080 with this method. However, it needs more epochs when training if I use \texttt{ImageDataGenerator}, usually I got about 0.96 training acc in 70 epochs without using it, but need to run about 250 epochs to reach the same training acc when using it. (batch\_size = 32)
\lstinputlisting[breaklines]{supervised-m.py}

\section{Self-Learning}
\n The method I use to implement self-learning is very similar to supervised learning. After training the model for the first round (only used labelled data), I predict all the unlabelled data, and pick the data whose max value in y\_train exceed threshold value then put it into x\_train, and iterate several times (iteration = 8). Because the last activation layer of the model is \texttt{softmax}, so the threshold should be close to 1 (In my implementation, it's setted to 0.98). \\
\n Another technique I used in this implementation is reset \texttt{nb\_epoch} after the first round, so that it won't train too many times.\\
\n By using self-learning, I got better score on Kaggle public set, the accuracy increase to 0.69540.
\lstinputlisting[breaklines]{auto-m.py}

\section{Autoencoder Clustering}
\n First, I design an deep autoencoder to reconstruct the feature of input images, the method I used to construct the autoencoder is just simply add some layers and the output of the model have the same dimension with the input images. Then the output of the middle layer is the encoded data, with new features.\\
\n Second, calculate the K-means boundary, and label those unlabelled data. The method I label those unlabelled data is to calculate the distance with the mean of each encoded feature, and pick the closest. then add the unlabelled data to x\_train.\\
\n Last, simply use the CNN network constructed in supervised learning to train all the data. The best score I got using this method is 0.68140. However, I think this is a good way with high potential, I think this will be better if I had enough time to try more models.
\lstinputlisting[breaklines]{self-m.py}

\newpage
\section{Result and Analysis}
\n Here I list the best score (Acc) I got on Kaggle public set.
\begin{itemize}
	\item Supervised (No datagen): 0.60820 (epoch =70, batch\_size = 32)
	\item Supervised (With datagen): 0.68080 (epoch = 250, batch\_size = 32)
	\item Self-learning (No datagen): 0.63960 (epoch = 70, batch\_size = 32, iteration = 8)
	\item Self-learning (With datagen): 0.69540 (epoch = 250, batch\_size = 32, iteration = 8)
	\item Autoencoder Clustering (With datagen): 0.68140 (epoch = 250, batch\_size = 128, add 5000 unlabelled)
\end{itemize}
\n I found that self-learning is useful, mostly have enhancement of 3\% to 5\%, and I think autoencoder is also a useful method if I add all the unlabelled data to x\_train and train more epochs, it may get even better result.
\end{document}




